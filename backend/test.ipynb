{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(\"app.db\")\n",
    "\n",
    "# Query to get prompt, response, and rating, excluding rows with NULL ratings\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    ds.section_content AS response,\n",
    "    ds.prompt_used AS prompt,\n",
    "    sf.rating AS rating\n",
    "FROM documentation_sections ds\n",
    "INNER JOIN section_feedback sf ON ds.id = sf.section_id\n",
    "WHERE sf.rating IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "# Load into DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Normalize ratings to a 0-1 scale\n",
    "df[\"reward\"] = df[\"rating\"] / 5.0  # Convert ratings from 1-5 to 0-1 scale\n",
    "\n",
    "# Save processed data\n",
    "df.to_csv(\"rlhf_preprocessed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>prompt</th>\n",
       "      <th>rating</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### 1. Project Overview\\n\\n**Project Summary**...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>**Technical Infrastructure**\\n================...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### 3. Component Organization\\n\\n**Project Str...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### 4. Dependencies and Requirements\\n\\n**Tech...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### 1. Overview and Purpose\\n\\n**Folder Overvi...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>### 2. Key Functions\\n\\n**Core Functionality**...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>### 3. Architecture\\n\\n**Design Patterns**\\n\\n...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>### 4. Inter-File Relationships\\n\\n**Component...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>### 1. Overview and Purpose\\n\\n#### Folder Ove...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>### 2. Key Functions\\n\\n#### 2.1 Core Function...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>### 3. Architecture\\n\\n#### Design Patterns\\n\\...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>### 5. Folder: inter_rs\\n\\nThe `inter_rs` fold...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>### 1. Overview and Purpose\\n\\n**Folder Overvi...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>### 2. Key Functions\\n\\n#### Core Functionalit...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>### 4. Inter-File Relationships\\n\\n**Component...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>### 1. Overview and Purpose\\n\\n**Folder Overvi...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>### 1. Project Overview\\n\\n**Project Summary**...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>### 2. Technical Infrastructure\\n\\n#### Develo...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>### 3. Component Organization\\n\\n#### Project ...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>### 4. Dependencies and Requirements\\n\\n**Tech...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>### Folder Overview\\n\\n**Overview and Purpose*...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>### Key Functions\\n\\n#### Database Session Man...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>### folder_architecture\\n\\nThe architecture of...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>### 4. folder_inter_rs\\n\\n#### Inter-File Rela...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>### Folder Dependencies\\n\\nThe `./uploaded_pro...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>### Folder Examples\\n\\n#### Overview\\n\\nThis s...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>### 1. Overview and Purpose\\n\\n**Folder Overvi...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>### 2. Key Functions\\n#### Core Functionality\\...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>### 3. Architecture\\n\\n**Design Patterns**\\n\\n...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>### 4. Inter-File Relationships\\n\\n**Component...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>### 5. folder_dependencies\\n\\n**External Depen...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>### 6. Code Snippets and Examples\\n\\n#### Comm...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>### 1. Overview and Purpose\\n\\n**Folder Overvi...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>### 2. Key Functions\\n\\n**Database Session Man...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>### 3. Architecture\\n\\n**Design Patterns**\\n\\n...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>### 4. Folder Inter-Relationships (Folder: ./u...</td>\n",
       "      <td>You are a technical documentation expert creat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             response  \\\n",
       "0   ### 1. Project Overview\\n\\n**Project Summary**...   \n",
       "1   **Technical Infrastructure**\\n================...   \n",
       "2   ### 3. Component Organization\\n\\n**Project Str...   \n",
       "3   ### 4. Dependencies and Requirements\\n\\n**Tech...   \n",
       "4   ### 1. Overview and Purpose\\n\\n**Folder Overvi...   \n",
       "5   ### 2. Key Functions\\n\\n**Core Functionality**...   \n",
       "6   ### 3. Architecture\\n\\n**Design Patterns**\\n\\n...   \n",
       "7   ### 4. Inter-File Relationships\\n\\n**Component...   \n",
       "8   ### 1. Overview and Purpose\\n\\n#### Folder Ove...   \n",
       "9   ### 2. Key Functions\\n\\n#### 2.1 Core Function...   \n",
       "10  ### 3. Architecture\\n\\n#### Design Patterns\\n\\...   \n",
       "11  ### 5. Folder: inter_rs\\n\\nThe `inter_rs` fold...   \n",
       "12  ### 1. Overview and Purpose\\n\\n**Folder Overvi...   \n",
       "13  ### 2. Key Functions\\n\\n#### Core Functionalit...   \n",
       "14  ### 4. Inter-File Relationships\\n\\n**Component...   \n",
       "15  ### 1. Overview and Purpose\\n\\n**Folder Overvi...   \n",
       "16  ### 1. Project Overview\\n\\n**Project Summary**...   \n",
       "17  ### 2. Technical Infrastructure\\n\\n#### Develo...   \n",
       "18  ### 3. Component Organization\\n\\n#### Project ...   \n",
       "19  ### 4. Dependencies and Requirements\\n\\n**Tech...   \n",
       "20  ### Folder Overview\\n\\n**Overview and Purpose*...   \n",
       "21  ### Key Functions\\n\\n#### Database Session Man...   \n",
       "22  ### folder_architecture\\n\\nThe architecture of...   \n",
       "23  ### 4. folder_inter_rs\\n\\n#### Inter-File Rela...   \n",
       "24  ### Folder Dependencies\\n\\nThe `./uploaded_pro...   \n",
       "25  ### Folder Examples\\n\\n#### Overview\\n\\nThis s...   \n",
       "26  ### 1. Overview and Purpose\\n\\n**Folder Overvi...   \n",
       "27  ### 2. Key Functions\\n#### Core Functionality\\...   \n",
       "28  ### 3. Architecture\\n\\n**Design Patterns**\\n\\n...   \n",
       "29  ### 4. Inter-File Relationships\\n\\n**Component...   \n",
       "30  ### 5. folder_dependencies\\n\\n**External Depen...   \n",
       "31  ### 6. Code Snippets and Examples\\n\\n#### Comm...   \n",
       "32  ### 1. Overview and Purpose\\n\\n**Folder Overvi...   \n",
       "33  ### 2. Key Functions\\n\\n**Database Session Man...   \n",
       "34  ### 3. Architecture\\n\\n**Design Patterns**\\n\\n...   \n",
       "35  ### 4. Folder Inter-Relationships (Folder: ./u...   \n",
       "\n",
       "                                               prompt  rating  reward  \n",
       "0   You are a technical documentation expert creat...       4     0.8  \n",
       "1   You are a technical documentation expert creat...       3     0.6  \n",
       "2   You are a technical documentation expert creat...       4     0.8  \n",
       "3   You are a technical documentation expert creat...       5     1.0  \n",
       "4   You are a technical documentation expert creat...       4     0.8  \n",
       "5   You are a technical documentation expert creat...       5     1.0  \n",
       "6   You are a technical documentation expert creat...       3     0.6  \n",
       "7   You are a technical documentation expert creat...       5     1.0  \n",
       "8   You are a technical documentation expert creat...       5     1.0  \n",
       "9   You are a technical documentation expert creat...       4     0.8  \n",
       "10  You are a technical documentation expert creat...       3     0.6  \n",
       "11  You are a technical documentation expert creat...       5     1.0  \n",
       "12  You are a technical documentation expert creat...       4     0.8  \n",
       "13  You are a technical documentation expert creat...       5     1.0  \n",
       "14  You are a technical documentation expert creat...       5     1.0  \n",
       "15  You are a technical documentation expert creat...       5     1.0  \n",
       "16  You are a technical documentation expert creat...       4     0.8  \n",
       "17  You are a technical documentation expert creat...       5     1.0  \n",
       "18  You are a technical documentation expert creat...       5     1.0  \n",
       "19  You are a technical documentation expert creat...       5     1.0  \n",
       "20  You are a technical documentation expert creat...       4     0.8  \n",
       "21  You are a technical documentation expert creat...       5     1.0  \n",
       "22  You are a technical documentation expert creat...       4     0.8  \n",
       "23  You are a technical documentation expert creat...       4     0.8  \n",
       "24  You are a technical documentation expert creat...       5     1.0  \n",
       "25  You are a technical documentation expert creat...       2     0.4  \n",
       "26  You are a technical documentation expert creat...       5     1.0  \n",
       "27  You are a technical documentation expert creat...       5     1.0  \n",
       "28  You are a technical documentation expert creat...       5     1.0  \n",
       "29  You are a technical documentation expert creat...       5     1.0  \n",
       "30  You are a technical documentation expert creat...       5     1.0  \n",
       "31  You are a technical documentation expert creat...       5     1.0  \n",
       "32  You are a technical documentation expert creat...       5     1.0  \n",
       "33  You are a technical documentation expert creat...       5     1.0  \n",
       "34  You are a technical documentation expert creat...       4     0.8  \n",
       "35  You are a technical documentation expert creat...       4     0.8  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Afnan Hussain\\.conda\\envs\\starcoder\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "c:\\Users\\Afnan Hussain\\.conda\\envs\\starcoder\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\Afnan Hussain\\.conda\\envs\\starcoder\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import ollama\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModelForCausalLM,get_linear_schedule_with_warmup\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from tqdm import tqdm\n",
    "from trl import PPOTrainer as TRLPPOTrainer\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentationRLHF:\n",
    "    def __init__(self, db_path='app.db'):\n",
    "        self.db_path = db_path\n",
    "        self._init_db()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Main device: {self.device}\")\n",
    "\n",
    "    def _init_db(self):\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS model_versions (\n",
    "                    id INTEGER PRIMARY KEY,\n",
    "                    model_name TEXT NOT NULL,\n",
    "                    version TEXT NOT NULL,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\")\n",
    "\n",
    "    def _get_training_data(self):\n",
    "        \"\"\"Retrieve training data from joined tables\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        query = \"\"\"\n",
    "            SELECT d.doc, f.feedback, f.rating\n",
    "            FROM documentation d\n",
    "            JOIN feedback f ON d.path = f.path AND d.level = f.level\n",
    "            WHERE f.rating IS NOT NULL\n",
    "            ORDER BY f.timestamp DESC\n",
    "        \"\"\"\n",
    "        return conn.execute(query).fetchall()\n",
    "\n",
    "    class RewardModelTrainer:\n",
    "        def __init__(self, db_path):\n",
    "            self.db_path = db_path\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            print(f\"Initializing Reward Model on {self.device}\")\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                self.model = self.model.to(self.device)\n",
    "                \n",
    "            print(f\"Reward model device: {next(self.model.parameters()).device}\")\n",
    "\n",
    "        def train(self):\n",
    "            data = DocumentationRLHF(self.db_path)._get_training_data()\n",
    "            texts = [d for d, _, _ in data]\n",
    "            ratings = torch.tensor([r/5 for _, _, r in data], \n",
    "                                dtype=torch.float32,\n",
    "                                device=self.device)\n",
    "            \n",
    "            # Check if there's enough data\n",
    "            if len(texts) == 0:\n",
    "                raise ValueError(\"No training data found. Please ensure the database contains valid data.\")\n",
    "            \n",
    "            optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps=50, \n",
    "                num_training_steps=3*max(1, len(texts)//16)  # Ensure at least 1 step\n",
    "            )\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in range(3):\n",
    "                total_loss = 0\n",
    "                # Shuffle the data for each epoch\n",
    "                texts_shuffled, ratings_shuffled = shuffle(texts, ratings)\n",
    "                \n",
    "                # Calculate the number of batches\n",
    "                num_batches = max(1, len(texts_shuffled) // 16)  # Ensure at least 1 batch\n",
    "                \n",
    "                for i in tqdm(range(0, len(texts_shuffled))):\n",
    "                    batch = self.tokenizer(\n",
    "                        texts_shuffled[i:i+16],\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=512,\n",
    "                        return_tensors=\"pt\"\n",
    "                    ).to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(**batch).logits.squeeze()\n",
    "                    loss = loss_fn(outputs, ratings_shuffled[i:i+16])\n",
    "                    \n",
    "                    # Gradient handling\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                # Calculate average loss\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    class CustomPPOTrainer:\n",
    "        def __init__(self, db_path, reward_model=None, reward_tokenizer=None):\n",
    "            self.db_path = db_path\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            # Model initialization\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "            # Use bfloat16 for better stability\n",
    "            torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "            \n",
    "            self.model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "                \"meta-llama/Llama-3.2-1B\",\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch_dtype,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            self.ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "                \"meta-llama/Llama-3.2-1B\",\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch_dtype,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "\n",
    "            # Reward model setup\n",
    "            if reward_model is None:\n",
    "                self.reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    \"saved_models/reward_model_rlhf\"\n",
    "                ).to(self.device)\n",
    "                self.reward_tokenizer = AutoTokenizer.from_pretrained(\"saved_models/reward_model_rlhf\")\n",
    "            else:\n",
    "                self.reward_model = reward_model.to(self.device)\n",
    "                self.reward_tokenizer = reward_tokenizer\n",
    "\n",
    "            self.reward_model.eval()\n",
    "\n",
    "            # PPO Configuration with stability enhancements\n",
    "            self.ppo_config = PPOConfig(\n",
    "                batch_size=1,\n",
    "                mini_batch_size=1,\n",
    "                gradient_accumulation_steps=1,\n",
    "                learning_rate=1e-6,\n",
    "                vf_coef=0.1,\n",
    "                kl_penalty=\"mse\",\n",
    "                adap_kl_ctrl=True,\n",
    "                target_kl=1.0,\n",
    "                init_kl_coef=0.2,\n",
    "                max_grad_norm=1.0,\n",
    "                cliprange=0.2,\n",
    "                cliprange_value=0.2,\n",
    "                optimize_cuda_cache=True,\n",
    "                use_score_scaling=True,\n",
    "                use_score_norm=True\n",
    "            )\n",
    "\n",
    "        def _calculate_rewards(self, generated_texts):\n",
    "            \"\"\"Calculate and normalize rewards\"\"\"\n",
    "            inputs = self.reward_tokenizer(\n",
    "                generated_texts,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                rewards = self.reward_model(**inputs).logits.squeeze()\n",
    "            \n",
    "            # Normalize rewards\n",
    "            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "            return rewards.clamp(-5, 5)\n",
    "\n",
    "        def train(self):\n",
    "            data = DocumentationRLHF(self.db_path)._get_training_data()\n",
    "            prompts = [\n",
    "                f\"Improve documentation:\\nOriginal: {d}\\nFeedback: {f}\\nImproved:\"\n",
    "                for d, f, _ in data\n",
    "            ]\n",
    "\n",
    "            # Filter and clean prompts\n",
    "            prompts = [p for p in prompts if len(p) > 10]\n",
    "\n",
    "            # Tokenize the prompts\n",
    "            def tokenize_function(examples):\n",
    "                return self.tokenizer(\n",
    "                    examples[\"query\"],\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors=\"pt\"  # Return PyTorch tensors directly\n",
    "                )\n",
    "\n",
    "            # Create a Dataset object from the prompts\n",
    "            dataset = Dataset.from_dict({\"query\": prompts})\n",
    "\n",
    "            # Tokenize the dataset\n",
    "            tokenized_dataset = dataset.map(tokenize_function, batched=True, batch_size=1)\n",
    "\n",
    "            # Initialize PPOTrainer with the tokenized dataset\n",
    "            ppo_trainer = PPOTrainer(\n",
    "                config=self.ppo_config,\n",
    "                model=self.model,\n",
    "                ref_model=self.ref_model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                dataset=tokenized_dataset  # Pass the tokenized dataset\n",
    "            )\n",
    "\n",
    "            generation_kwargs = {\n",
    "                \"do_sample\": True,\n",
    "                \"temperature\": 0.8,\n",
    "                \"top_p\": 0.92,\n",
    "                \"top_k\": 50,\n",
    "                \"max_new_tokens\": 256,\n",
    "                \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"repetition_penalty\": 1.15,\n",
    "                \"no_repeat_ngram_size\": 3\n",
    "            }\n",
    "\n",
    "            for epoch in range(3):\n",
    "                for batch in tqdm(ppo_trainer.dataloader, desc=f\"PPO Epoch {epoch+1}\"):\n",
    "                    try:\n",
    "                        \n",
    "                        # Move batch tensors to the correct device\n",
    "                        query_tensors = batch[\"input_ids\"]\n",
    "                        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "                        # Generate responses\n",
    "                        response_tensors = ppo_trainer.generate(\n",
    "                            query_tensor=query_tensors,\n",
    "                            return_prompt=False,\n",
    "                            **generation_kwargs\n",
    "                        )\n",
    "\n",
    "                        # Decode responses\n",
    "                        generated_texts = [\n",
    "                            self.tokenizer.decode(r, skip_special_tokens=True)\n",
    "                            for r in response_tensors\n",
    "                        ]\n",
    "\n",
    "                        # Calculate rewards\n",
    "                        rewards = self._calculate_rewards(generated_texts)\n",
    "\n",
    "                        # PPO step with stability checks\n",
    "                        stats = ppo_trainer.step(\n",
    "                            query_tensors,\n",
    "                            response_tensors,\n",
    "                            rewards\n",
    "                        )\n",
    "                        \n",
    "                        # Check for NaN and reset if needed\n",
    "                        if torch.isnan(torch.tensor(list(stats.values()))).any():\n",
    "                            print(\"NaN detected in stats - resetting gradients\")\n",
    "                            ppo_trainer.optimizer.zero_grad()\n",
    "                            \n",
    "                    except RuntimeError as e:\n",
    "                        if 'nan' in str(e).lower():\n",
    "                            print(\"NaN detected - skipping batch\")\n",
    "                            continue\n",
    "                        raise\n",
    "\n",
    "    \n",
    "\n",
    "    def log_documentation(self, user_id: str, path: str, doc: str, prompt: str,\n",
    "                          project_name: str, level: str, root_path: str):\n",
    "        \"\"\"Log generated documentation in the database\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\"\"\"\n",
    "                INSERT INTO documentation\n",
    "                (user_id, path, doc, prompt, project_name, level, root_path)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", (user_id, path, doc, prompt, project_name, level, root_path))\n",
    "\n",
    "    def add_feedback(self, user_id: str, path: str, level: str, feedback: str, rating: int):\n",
    "        \"\"\"Add user feedback to the database\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\"\"\"\n",
    "                INSERT INTO feedback\n",
    "                (user_id, path, level, feedback, rating)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "            \"\"\", (user_id, path, level, feedback, rating))\n",
    "\n",
    "    def full_training_pipeline(self):\n",
    "        \"\"\"Complete training workflow with model saving.\"\"\"\n",
    "        # 1. Train reward model\n",
    "        #reward_trainer = self.RewardModelTrainer(self.db_path)\n",
    "        #reward_trainer.train()\n",
    "        \n",
    "        # Save the trained reward model along with its tokenizer\n",
    "        reward_model_save_path = \"saved_models/reward_model_rlhf\"\n",
    "        #reward_trainer.model.save_pretrained(reward_model_save_path)\n",
    "        #reward_trainer.tokenizer.save_pretrained(reward_model_save_path)\n",
    "        #print(f\"Reward model and tokenizer saved to {reward_model_save_path}\")\n",
    "    \n",
    "        # 2. Train with PPO\n",
    "        # Load saved reward model and tokenizer\n",
    "        reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_save_path)\n",
    "        reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_save_path)\n",
    "        \n",
    "        # Pass the stored reward model and tokenizer to the PPO trainer\n",
    "        ppo_trainer = self.CustomPPOTrainer(self.db_path, reward_model=reward_model, reward_tokenizer=reward_tokenizer)\n",
    "        ppo_trainer.train()\n",
    "        \n",
    "        # Save the PPO model\n",
    "        ppo_model_save_path = \"saved_models/ppo_model_rlhf\"\n",
    "        ppo_trainer.model.save_pretrained(ppo_model_save_path)\n",
    "        ppo_trainer.tokenizer.save_pretrained(ppo_model_save_path)\n",
    "        print(f\"PPO model saved to {ppo_model_save_path}\")\n",
    "    \n",
    "        # 3. Log new model version\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\"\"\"\n",
    "                INSERT INTO model_versions (model_name, version)\n",
    "                VALUES ('documentation_generator', 'rlhf-v1')\n",
    "            \"\"\")\n",
    "            conn.commit()\n",
    "        print(\"Logged new model version to the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main device: cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-3.2-1B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-3.2-1B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "c:\\Users\\Afnan Hussain\\.conda\\envs\\starcoder\\lib\\site-packages\\trl\\trainer\\ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Afnan Hussain\\.conda\\envs\\starcoder\\lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot locate reference to <class '__main__.DocumentationRLHF.CustomPPOTrainer'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "c:\\Users\\Afnan Hussain\\.conda\\envs\\starcoder\\lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot pickle <class '__main__.DocumentationRLHF.CustomPPOTrainer'>: __main__.DocumentationRLHF.CustomPPOTrainer has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb64e72dc0c44ca814e59aa0463f383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Afnan Hussain\\.conda\\envs\\starcoder\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n",
      "PPO Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\Afnan Hussain\\.conda\\envs\\starcoder\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:53: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "PPO Epoch 1:   0%|          | 0/7 [23:53<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Batch size (1) does not match number of examples - but got 512 for: queries",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(rlhf\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mrlhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 300\u001b[0m, in \u001b[0;36mDocumentationRLHF.full_training_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# Pass the stored reward model and tokenizer to the PPO trainer\u001b[39;00m\n\u001b[0;32m    299\u001b[0m ppo_trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCustomPPOTrainer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_path, reward_model\u001b[38;5;241m=\u001b[39mreward_model, reward_tokenizer\u001b[38;5;241m=\u001b[39mreward_tokenizer)\n\u001b[1;32m--> 300\u001b[0m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# Save the PPO model\u001b[39;00m\n\u001b[0;32m    303\u001b[0m ppo_model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_models/ppo_model_rlhf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 243\u001b[0m, in \u001b[0;36mDocumentationRLHF.CustomPPOTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_rewards(generated_texts)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# PPO step with stability checks\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Check for NaN and reset if needed\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(stats\u001b[38;5;241m.\u001b[39mvalues())))\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[1;32mc:\\Users\\Afnan Hussain\\.conda\\envs\\starcoder\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\Afnan Hussain\\.conda\\envs\\starcoder\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:680\u001b[0m, in \u001b[0;36mPPOTrainer.step\u001b[1;34m(self, queries, responses, scores, response_masks)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;124;03mRun a PPO optimisation step given a list of queries, model responses, and rewards.\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;124;03m    `dict[str, Any]`: A summary of the training statistics\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    678\u001b[0m bs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m--> 680\u001b[0m queries, responses, scores, response_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_safety_checker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_masks\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(scores, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_device)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_score_scaling:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;66;03m# Score scaling\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Afnan Hussain\\.conda\\envs\\starcoder\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:635\u001b[0m, in \u001b[0;36mPPOTrainer._step_safety_checker\u001b[1;34m(self, batch_size, queries, responses, scores, masks)\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElements in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be tensors - got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor_list[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor_list) \u001b[38;5;241m!=\u001b[39m batch_size:\n\u001b[1;32m--> 635\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    636\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match number of examples - but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tensor_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    637\u001b[0m         )\n\u001b[0;32m    639\u001b[0m \u001b[38;5;66;03m# add queries, scores and responses on the correct device\u001b[39;00m\n\u001b[0;32m    640\u001b[0m queries \u001b[38;5;241m=\u001b[39m [tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_device) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m queries]\n",
      "\u001b[1;31mValueError\u001b[0m: Batch size (1) does not match number of examples - but got 512 for: queries"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    rlhf = DocumentationRLHF()\n",
    "    print(rlhf.device)\n",
    "   \n",
    "    # Run training\n",
    "    rlhf.full_training_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Some weights of the model checkpoint at saved_models/ppo_model_rlhf were not used when initializing LlamaForCausalLM: {'v_head.summary.weight', 'v_head.summary.bias'}\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      " What is the importance of AI in healthcare?!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "model_path = \"saved_models/ppo_model_rlhf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                llm_int8_enable_fp32_cpu_offload=False\n",
    "            )\n",
    "\n",
    "# Load RLHF-trained model with explicit quantization type\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_response(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the trained model\n",
    "prompt = \"What is the importance of AI in healthcare?\"\n",
    "response = generate_response(prompt)\n",
    "\n",
    "print(\"Generated Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_load_in_4bit': True, '_load_in_8bit': False, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'fp4', 'bnb_4bit_use_double_quant': False, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': None, 'llm_int8_threshold': 6.0, 'load_in_4bit': True, 'load_in_8bit': False, 'quant_method': 'bitsandbytes'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "model_path = \"saved_models/ppo_model_rlhf/config.json\"\n",
    "\n",
    "with open(model_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(config.get(\"quantization_config\", \"No quantization config found\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeinsight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
